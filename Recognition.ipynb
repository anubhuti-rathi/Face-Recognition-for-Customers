{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0wwvg_SPn0Y",
        "outputId": "2955ff8e-74b0-4773-b3ee-a2ecb3a840c4"
      },
      "source": [
        "# import libraries\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# load serialized face detector\n",
        "print(\"Loading Face Detector...\")\n",
        "protoPath = \"/content/drive/MyDrive/Myphotos/model/deploy.prototxt\"\n",
        "modelPath = \"/content/drive/MyDrive/Myphotos/model/res10_300x300_ssd_iter_140000_fp16 (1).caffemodel\"\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
        "\n",
        "# load serialized face embedding model\n",
        "print(\"Loading Face Recognizer...\")\n",
        "embedder = cv2.dnn.readNetFromTorch(\"/content/drive/MyDrive/Myphotos/model/openface_nn4.small2.v1.t7\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Face Detector...\n",
            "Loading Face Recognizer...\n",
            "Quantifying Faces...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5C7ss3KzONH",
        "outputId": "ebc66818-0a9d-4aa9-c5be-07d153e278eb"
      },
      "source": [
        "# grab the paths to the input images in our dataset\n",
        "print(\"Quantifying Faces...\")\n",
        "imagePaths = list(paths.list_images(\"/content/drive/MyDrive/Myphotos/FaceDataset\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quantifying Faces...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hlb2bpePnuy"
      },
      "source": [
        "knownEmbeddings = []\n",
        "knownNames = []\n",
        "\n",
        "# initialize the total number of faces processed\n",
        "total = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoaRYqRBRwV0"
      },
      "source": [
        "TRAINING_SIZE=50\n",
        "ThreshTime = 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR_mAhVzeDEZ"
      },
      "source": [
        "def maxFreq(resp):\n",
        "\tMaxName={}\n",
        "\tfor each in resp:\n",
        "\t\tif each in MaxName:\n",
        "\t\t\tMaxName[each]= MaxName[each]+1\n",
        "\t\telse:\n",
        "\t\t\tMaxName[each]= 0\n",
        "\tMax = 0\n",
        "\tind =str()\n",
        "\t\n",
        "\tfor key, val in MaxName.items():\n",
        "\t\tif(Max<val):\n",
        "\t\t\tMax = val\n",
        "\t\t\tind = key\n",
        "\t\telse:\n",
        "\t\t\tpass\n",
        "\treturn ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO2tf4tXY7Ua"
      },
      "source": [
        "def embeddingsGenrerator():\n",
        "  total=0\n",
        "  for (i, imagePath) in enumerate(imagePaths[:5]):\n",
        "    # extract the person name from the image path\n",
        "    if (i%50 == 0):\n",
        "      print(\"Processing image {}/{}\".format(i, len(imagePaths)))\n",
        "    name = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "    # load the image, resize it to have a width of 600 pixels (while maintaining the aspect ratio), and then grab the image dimensions\n",
        "    image = cv2.imread(imagePath)\n",
        "    image = imutils.resize(image, width=600)\n",
        "    #cv2_imshow(image)\n",
        "    #print(image.shape)\n",
        "    (h, w) = image.shape[:2]\n",
        "\n",
        "    #imm=cv2.resize(image,(300,300))\n",
        "    #cv2_imshow(image)\n",
        "    #print(image.shape)\n",
        "\n",
        "    # construct a blob from the image\n",
        "    imageBlob = cv2.dnn.blobFromImage(\n",
        "      cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
        "      (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
        "\n",
        "    # apply OpenCV's deep learning-based face detector to localize faces in the input image\n",
        "    detector.setInput(imageBlob)\n",
        "    detections = detector.forward()\n",
        "    print(detections)\n",
        "    print(detections.shape)\n",
        "    print(len(detections))\n",
        "\n",
        "    # ensure at least one face was found\n",
        "    if len(detections) > 0:\n",
        "      # we're making the assumption that each image has only ONE face, so find the bounding box with the largest probability\n",
        "      i = np.argmax(detections[0, 0, :, 2])\n",
        "      confidence = detections[0, 0, i, 2]\n",
        "\n",
        "      # ensure that the detection with the largest probability also means our minimum probability test (thus helping filter out weak detections)\n",
        "      if confidence > 0.5:\n",
        "        # compute the (x, y)-coordinates of the bounding box for the face\n",
        "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "        # extract the face ROI and grab the ROI dimensions\n",
        "        face = image[startY:endY, startX:endX]\n",
        "        (fH, fW) = face.shape[:2]\n",
        "\n",
        "        # ensure the face width and height are sufficiently large\n",
        "        if fW < 20 or fH < 20:\n",
        "          continue\n",
        "\n",
        "        # construct a blob for the face ROI, then pass the blob through our face embedding model to obtain the 128-d quantification of the face\n",
        "        faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
        "          (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
        "        embedder.setInput(faceBlob)\n",
        "        vec = embedder.forward()\n",
        "\n",
        "        # add the name of the person + corresponding face embedding to their respective lists\n",
        "        knownNames.append(name)\n",
        "        knownEmbeddings.append(vec.flatten())\n",
        "        total += 1\n",
        "      else:\n",
        "        \n",
        "\n",
        "  # dump the facial embeddings + names to disk\n",
        "  print(\"[INFO] serializing {} encodings...\".format(total))\n",
        "  data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
        "  f = open(\"/content/drive/MyDrive/Myphotos/output/embeddings.pickle\", \"wb\")\n",
        "  f.write(pickle.dumps(data))\n",
        "  f.close()\n",
        "\n",
        "def trainSVM():\n",
        "  print(\"starting training Process\")\n",
        "  data = pickle.loads(open(\"/content/drive/MyDrive/Myphotos/output/embeddings.pickle\", \"rb\").read())\n",
        "  le = LabelEncoder()\n",
        "  labels = le.fit_transform(data[\"names\"])\n",
        "\n",
        "  print(\"[INFO] training model...\")\n",
        "  recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
        "  #print(data['embeddings'])\n",
        "  if data['embeddings']:\n",
        "    recognizer.fit(data[\"embeddings\"], labels)\n",
        "\n",
        "  print(\"Svm trained\")\n",
        "  f = open(\"/content/drive/MyDrive/Myphotos/output/recognizer.pickle\", \"wb\")\n",
        "  f.write(pickle.dumps(recognizer))\n",
        "  f.close()\n",
        "\n",
        "  f = open(\"/content/drive/MyDrive/Myphotos/output/le.pickle\", \"wb\")\n",
        "  f.write(pickle.dumps(le))\n",
        "  f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByDgHm9QRTQO"
      },
      "source": [
        "embeddingsGenrerator()\n",
        "trainSVM()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}