{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GenerateEmbeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLjOKRjq3L3o"
      },
      "source": [
        "from imutils import paths\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYwrTpGU340u"
      },
      "source": [
        "protoPath = os.path.sep.join(\"/content/drive/MyDrive/Myphotos/model/deploy.prototxt\")\n",
        "modelPath = os.path.sep.join(\"/content/drive/MyDrive/Myphotos/model/res10_300x300_ssd_iter_140000_fp16 (1).caffemodel\")\n",
        "detector = cv2.dnn.readNetFromCaffe(\"/content/drive/MyDrive/Myphotos/model/deploy.prototxt\" , \"/content/drive/MyDrive/Myphotos/model/res10_300x300_ssd_iter_140000_fp16 (1).caffemodel\")\n",
        "embedder = cv2.dnn.readNetFromTorch(\"/content/drive/MyDrive/Myphotos/model/openface_nn4.small2.v1.t7\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcULcLE53-3W",
        "outputId": "8c8a51ff-0a45-446d-88c7-b85e1073f946"
      },
      "source": [
        "imagePaths = list(paths.list_images(\"/content/drive/MyDrive/Myphotos/FaceDataset\"))\n",
        "print(imagePaths[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/MyDrive/Myphotos/FaceDataset/00299.jpg', '/content/drive/MyDrive/Myphotos/FaceDataset/00300.jpg', '/content/drive/MyDrive/Myphotos/FaceDataset/00301.jpg', '/content/drive/MyDrive/Myphotos/FaceDataset/00302.jpg', '/content/drive/MyDrive/Myphotos/FaceDataset/00303.jpg', '/content/drive/MyDrive/Myphotos/FaceDataset/00304.jpg', '/content/drive/MyDrive/Myphotos/FaceDataset/00305.jpg', '/content/drive/MyDrive/Myphotos/FaceDataset/00306.jpg', '/content/drive/MyDrive/Myphotos/FaceDataset/00307.jpg', '/content/drive/MyDrive/Myphotos/FaceDataset/00308.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRljLW404Gfy"
      },
      "source": [
        "# initialize our lists of extracted facial embeddings and\n",
        "# corresponding people names\n",
        "knownEmbeddings = []\n",
        "knownNames = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwPazwkj4U8X"
      },
      "source": [
        "# initialize the total number of faces processed\n",
        "total=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7pb2YxbA9iP"
      },
      "source": [
        "Blob from Image(Binary Large Objects)\n",
        "\n",
        "Audio, Video and Image data. Group of connected pixels in a binary image which shares a common property.\n",
        "\n",
        "In the field of deep learning and classification, we preprocess the data in two ways:\n",
        "\n",
        "Mean Subtraction\n",
        "\n",
        "Scaling by some factor\n",
        "\n",
        "Open cv's DNN consists of two functions for pre processing for classification\n",
        "\n",
        "cv2.dnn.blobFromImages and cv2.dnn.blobFromImage\n",
        "\n",
        "Steps in Pre Processing:\n",
        "\n",
        "1.Decrease the average value\n",
        "\n",
        "2.Zoom\n",
        "\n",
        "3.Channel exchange"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs_IWzqL4brf"
      },
      "source": [
        "# loop over the image paths\n",
        "for (i, imagePath) in enumerate(imagePaths):\n",
        "\t# extract the person name from the image path\n",
        "\tprint(\"[INFO] processing image {}/{}\".format(i + 1,\n",
        "\t\tlen(imagePaths)))\n",
        "\tname = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t# load the image, resize it to have a width of 600 pixels (while\n",
        "\t# maintaining the aspect ratio), and then grab the image\n",
        "\t# dimensions\n",
        "\timage = cv2.imread(imagePath)\n",
        "\tif(image is None):\n",
        "\t\tpass\n",
        "\telse:\n",
        "\t\t\n",
        "\t\timage = imutils.resize(image, width=600)\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\n",
        "\t\t# construct a blob from the image\n",
        "\t\timageBlob = cv2.dnn.blobFromImage(\n",
        "\t\t\tcv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
        "\t\t\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
        "\n",
        "\t\t# apply OpenCV's deep learning-based face detector to localize\n",
        "\t\t# faces in the input image\n",
        "\t\tdetector.setInput(imageBlob)\n",
        "\t\tdetections = detector.forward()\n",
        "\n",
        "\t\t# ensure at least one face was found\n",
        "\t\tif len(detections) > 0:\n",
        "\t\t\t# we're making the assumption that each image has only ONE\n",
        "\t\t\t# face, so find the bounding box with the largest probability\n",
        "\t\t\ti = np.argmax(detections[0, 0, :, 2])\n",
        "\t\t\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t\t\t# ensure that the detection with the largest probability also\n",
        "\t\t\t# means our minimum probability test (thus helping filter out\n",
        "\t\t\t# weak detections)\n",
        "\t\t\tif confidence > 0.95:\n",
        "\t\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t\t# the face\n",
        "\t\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t\t# extract the face ROI and grab the ROI dimensions\n",
        "\t\t\t\tface = image[startY:endY, startX:endX]\n",
        "\t\t\t\tcv2_imshow(face)\n",
        "\t\t\t\tif(cv2.waitKey(1) and 0xFF==27):\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\t(fH, fW) = face.shape[:2]\n",
        "\n",
        "\t\t\t\t# ensure the face width and height are sufficiently large\n",
        "\t\t\t\tif fW < 20 or fH < 20:\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t# construct a blob for the face ROI, then pass the blob\n",
        "\t\t\t\t# through our face embedding model to obtain the 128-d\n",
        "\t\t\t\t# quantification of the face\n",
        "\t\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
        "\t\t\t\t\t(96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
        "\t\t\t\tembedder.setInput(faceBlob)\n",
        "\t\t\t\tvec = embedder.forward()\n",
        "\n",
        "\t\t\t\t# add the name of the person + corresponding face\n",
        "\t\t\t\t# embedding to their respective lists\n",
        "\t\t\t\tknownNames.append(name)\n",
        "\t\t\t\tknownEmbeddings.append(vec.flatten())\n",
        "\t\t\t\ttotal += 1\n",
        "\n",
        "print(\"[INFO] serializing {} encodings...\".format(total))\n",
        "data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
        "f = open(\"/content/drive/MyDrive/Myphotos/output/embeddings.pickle\", \"wb\")\n",
        "data.append(pickle.dumps(data))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}